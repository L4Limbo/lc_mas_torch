{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "from gensim.models import KeyedVectors\n",
                "from gensim import matutils\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Vocabulary and Vectors\n"
                    ]
                }
            ],
            "source": [
                "print('Loading Vocabulary and Vectors')\n",
                "word2vec = KeyedVectors.load_word2vec_format(\n",
                "    'data/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
                "vocabulary = json.load(open('data/processed_data/processed_data.json', 'r'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(1.6194, grad_fn=<MseLossBackward0>)"
                        ]
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "scores = rouge.get_scores(\n",
                "    generated_summary, reference_summary\n",
                ")\n",
                "print(scores)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "metadata": {},
            "outputs": [],
            "source": [
                "def word2vec_emb(sequence, word2vec, vocabulary):\n",
                "    \n",
                "    seq_tokens = []\n",
                "    \n",
                "    for key in sequence:\n",
                "        try:\n",
                "            seq_tokens.append(vocabulary['ind2word'][str(key.item())])\n",
                "        except:\n",
                "            seq_tokens.append(vocabulary['ind2word'][list(\n",
                "                vocabulary['ind2word'].keys())[-1]])\n",
                "    \n",
                "    vectorized = []\n",
                "    for word in seq_tokens:\n",
                "        try:\n",
                "            vectorized.append(word2vec[word])\n",
                "        except:\n",
                "            vectorized.append(np.zeros(300,))\n",
                "    \n",
                "    return np.array(vectorized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "metadata": {},
            "outputs": [],
            "source": [
                "def similarity_cosine(vec1, vec2):\n",
                "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
                "    print(cosine_similarity)\n",
                "    print(spatial.distance.cosine(vec1, vec2))\n",
                "    return cosine_similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 104,
            "metadata": {},
            "outputs": [],
            "source": [
                "def reward(target, generated, word2vec, vocabulary):\n",
                "    '''\n",
                "    Calculate the reward for the generated text\n",
                "    '''\n",
                "    target = word2vec_emb(target, word2vec, vocabulary)\n",
                "    generated = word2vec_emb(generated, word2vec, vocabulary)\n",
                "    \n",
                "    reward = similarity_cosine(target.mean(axis=0), generated.mean(axis=0))\n",
                "\n",
                "    return reward"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 123,
            "metadata": {},
            "outputs": [],
            "source": [
                "answer = torch.tensor([[6100, 4948, 6066, 1336, 4264, 4093,   37, 3343, 2467, 5527, 3396, 4326,\n",
                "          219, 5560, 5845, 1464, 3149, 2805, 4154, 1899, 5642]])\n",
                "\n",
                "question = torch.tensor([[6100, 2440, 5128, 5570,  511, 2483, 4828, 3564,  282, 3869, 2614,    7,\n",
                "         4783, 4595, 5613, 1385, 6032, 4369,  286, 2259, 1900]])\n",
                "\n",
                "summary = torch.tensor([[6100, 5955, 2642, 6021, 1982, 3081, 3554, 3802, 4577, 5298, 5555, 1110,\n",
                "         4515, 2661, 2299, 4606, 4956, 4007, 3281, 2625, 2206, 4335, 2730, 2925,\n",
                "         4523, 1231, 4269,  427,  861, 4104, 2630, 2499, 5430, 4043, 5146, 4595,\n",
                "         3772, 5545, 4852, 3325, 4324]])\n",
                "\n",
                "gtSummary = torch.tensor([[405,  333,  880,  188,  624,  194,  996,  293,  244, 6099, 3275,   99,\n",
                "           25,  244,  754,  636,  936, 1461, 3653,   51,   55,   56,   57,    1,\n",
                "           58,    1,   59,    1,   60,   52,  244, 1367, 6099, 1368, 3654, 2424,\n",
                "           76,   51, 3655,   52]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import spatial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 128,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.7565139467498903\n",
                        "0.6523351061715779\n"
                    ]
                }
            ],
            "source": [
                "# torch.cat((answer[0], question[0]))\n",
                "print(reward(target=torch.cat((answer[0], question[0])), generated=summary[0], word2vec=word2vec, vocabulary=vocabulary))\n",
                "print(reward(target=gtSummary[0], generated=summary[0], word2vec=word2vec, vocabulary=vocabulary))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from rouge import Rouge\n",
                "def rouge_scores(target, generated, word2vec, vocabulary):\n",
                "    rouge = Rouge()\n",
                "    tar = []\n",
                "    ref = []\n",
                "\n",
                "    for key in target:\n",
                "        try:\n",
                "            if vocabulary['ind2word'][str(key.item())] !='UNK':\n",
                "                tar.append(vocabulary['ind2word'][str(key.item())])\n",
                "        except:\n",
                "            pass\n",
                "\n",
                "    for key in generated:\n",
                "        try:\n",
                "            if vocabulary['ind2word'][str(key.item())] !='UNK':\n",
                "                ref.append(vocabulary['ind2word'][str(key.item())])\n",
                "        except:\n",
                "            pass\n",
                "\n",
                "    return rouge.get_scores(' '.join(ref), ' '.join(tar), avg=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "40 visible one-half calling retinal open-angle missing elbows street hopeless absolutely S loss Depakene TEE acid minimal double recurrences opposite Cutting tends efforts settings hepatitis reflexes heart-lung Name 1 Oxygen endoscopy if Autonomic beat companies cervical Nonne-Milroy separates Fragile osteochondrodysplasias abnormally\n",
                        "- - - - - - \n",
                        "40 running electrical Recurrent barley amyloid Rapid senses containers Evidence Hospitalization testing Joseph FODMAP x-rays fasting coated Southeast psoralen Sharing natural G pacing mouthwashes Rasagiline While psychiatric resistance new lights relieving babies Involved specially firm beat relationships instability cookies colloidal mechanisms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'rouge-1': {'r': 0.025, 'p': 0.025, 'f': 0.024999995000001003},\n",
                            " 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
                            " 'rouge-l': {'r': 0.025, 'p': 0.025, 'f': 0.024999995000001003}}"
                        ]
                    },
                    "execution_count": 158,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "rouge_scores(torch.cat((answer[0], question[0])), summary[0], word2vec, vocabulary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 138,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'rouge-1': {'r': 0.7142857142857143, 'p': 1.0, 'f': 0.8333333284722222},\n",
                            " 'rouge-2': {'r': 0.6666666666666666, 'p': 1.0, 'f': 0.7999999952000001},\n",
                            " 'rouge-l': {'r': 0.7142857142857143, 'p': 1.0, 'f': 0.8333333284722222}}"
                        ]
                    },
                    "execution_count": 138,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Rouge().get_scores('I go to the beach','I go to the beach every data', avg=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 242,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "from six.moves import range\n",
                "\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "import nltk\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.autograd import Variable\n",
                "import torchvision\n",
                "\n",
                "import lc_options\n",
                "from utils import lc_utilities as utils\n",
                "from rouge import Rouge"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 244,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading json file: data/visdial/chat_processed_params.json\n",
                        "Vocab size with <START>, <END>: 8333\n"
                    ]
                }
            ],
            "source": [
                "params = {\n",
                "    'inputJson': \"data/visdial/chat_processed_params.json\",\n",
                "    'useGPU': False,\n",
                "    \n",
                "    # A-Bot checkpoint\n",
                "    'startFrom': \"./checkpoints/vis_a/abot_ep_1.vd\",\n",
                "    \n",
                "    # Q-Bot checkpoint should given if interactive dialog is required\n",
                "    'qstartFrom': \"./checkpoints/vis_q/qbot_ep_0.vd\",\n",
                "    \n",
                "    'beamSize': 5,\n",
                "}\n",
                "\n",
                "# RNG seed\n",
                "manualSeed = 1597\n",
                "random.seed(manualSeed)\n",
                "torch.manual_seed(manualSeed)\n",
                "if params['useGPU']:\n",
                "    torch.cuda.manual_seed_all(manualSeed)\n",
                "\n",
                "print('Loading json file: ' + params['inputJson'])\n",
                "with open(params['inputJson'], 'r') as fileId:\n",
                "    info = json.load(fileId)\n",
                "\n",
                "wordCount = len(info['word2ind'])\n",
                "# Add <START> and <END> to vocabulary\n",
                "info['word2ind']['<START>'] = wordCount + 1\n",
                "info['word2ind']['<END>'] = wordCount + 2\n",
                "startToken = info['word2ind']['<START>']\n",
                "endToken = info['word2ind']['<END>']\n",
                "# Padding token is at index 0\n",
                "vocabSize = wordCount + 3\n",
                "print('Vocab size with <START>, <END>: %d' % vocabSize)\n",
                "\n",
                "# Construct the reverse map\n",
                "info['ind2word'] = {\n",
                "    int(ind): word\n",
                "    for word, ind in info['word2ind'].items()\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 245,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def loadModel(params, agent='abot'):\n",
                "    # should be everything used in encoderParam, decoderParam below\n",
                "    encoderOptions = [\n",
                "        'encoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
                "        'useHistory', 'numRounds', 'dropout', 'useSumm'\n",
                "    ]\n",
                "    decoderOptions = [\n",
                "        'decoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
                "        'dropout'\n",
                "    ]\n",
                "    modelOptions = encoderOptions + decoderOptions\n",
                "\n",
                "    mdict = None\n",
                "    gpuFlag = params['useGPU']\n",
                "    startArg = 'startFrom' if agent == 'abot' else 'qstartFrom'\n",
                "    assert params[startArg], \"Need checkpoint for {}\".format(agent)\n",
                "\n",
                "    if params[startArg]:\n",
                "        print('Loading model (weights and config) from {}'.format(\n",
                "            params[startArg]))\n",
                "\n",
                "        if gpuFlag:\n",
                "            mdict = torch.load(params[startArg])\n",
                "        else:\n",
                "            mdict = torch.load(params[startArg],\n",
                "                map_location=lambda storage, location: storage)\n",
                "\n",
                "        # Model options is a union of standard model options defined\n",
                "        # above and parameters loaded from checkpoint\n",
                "        modelOptions = list(set(modelOptions).union(set(mdict['params'])))\n",
                "        for opt in modelOptions:\n",
                "            if opt not in params:\n",
                "                params[opt] = mdict['params'][opt]\n",
                "\n",
                "            elif params[opt] != mdict['params'][opt]:\n",
                "                # Parameters are not overwritten from checkpoint\n",
                "                pass\n",
                "\n",
                "    # Initialize model class\n",
                "    encoderParam = {k: params[k] for k in encoderOptions}\n",
                "    decoderParam = {k: params[k] for k in decoderOptions}\n",
                "\n",
                "    encoderParam['startToken'] = encoderParam['vocabSize'] - 2\n",
                "    encoderParam['endToken'] = encoderParam['vocabSize'] - 1\n",
                "    decoderParam['startToken'] = decoderParam['vocabSize'] - 2\n",
                "    decoderParam['endToken'] = decoderParam['vocabSize'] - 1\n",
                "\n",
                "    if agent == 'abot':\n",
                "        encoderParam['type'] = params['encoder']\n",
                "        decoderParam['type'] = params['decoder']\n",
                "        encoderParam['isAnswerer'] = True\n",
                "        from lc.models.lc_answerer import Answerer\n",
                "        model = Answerer(encoderParam, decoderParam)\n",
                "\n",
                "    elif agent == 'qbot':\n",
                "        encoderParam['type'] = params['qencoder']\n",
                "        decoderParam['type'] = params['qdecoder']\n",
                "        encoderParam['isAnswerer'] = False\n",
                "        encoderParam['useSumm'] = False\n",
                "        from lc.models.lc_questioner import Questioner\n",
                "        model = Questioner(\n",
                "            encoderParam,\n",
                "            decoderParam,\n",
                "            summGenSize=60)\n",
                "\n",
                "    # if params['useGPU']:\n",
                "    #     model.cuda()\n",
                "\n",
                "    if mdict:\n",
                "        model.load_state_dict(mdict['model'])\n",
                "        \n",
                "    print(\"Loaded agent {}\".format(agent))\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 246,
            "metadata": {},
            "outputs": [],
            "source": [
                "aBot = None\n",
                "qBot = None\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 247,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model (weights and config) from ./checkpoints/vis_a/abot_ep_1.vd\n",
                        "Encoder: hre-ques-lateim-hist\n",
                        "Decoder: gen\n",
                        "Loaded agent abot\n",
                        "Loading model (weights and config) from ./checkpoints/vis_q/qbot_ep_0.vd\n",
                        "Encoder: hre-ques-lateim-hist\n",
                        "Decoder: gen\n",
                        "Loaded agent qbot\n"
                    ]
                }
            ],
            "source": [
                "# load aBot\n",
                "if params['startFrom']:\n",
                "    aBot = loadModel(params, 'abot')\n",
                "    assert aBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
                "    aBot.eval()\n",
                "    \n",
                "# load qBot\n",
                "if params['qstartFrom']:\n",
                "    qBot = loadModel(params, 'qbot')\n",
                "    assert qBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
                "    qBot.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 248,
            "metadata": {},
            "outputs": [],
            "source": [
                "ind_map = lambda words: np.array([info['word2ind'].get(word, info['word2ind']['UNK']) \n",
                "                                  for word in words], dtype='int64')\n",
                "\n",
                "tokenize = lambda string: ['<START>'] + word_tokenize(string) + ['<END>']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 249,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions for converting tensors to words\n",
                "to_str_pred = lambda w, l: str(\" \".join([info['ind2word'][x] for x in list( filter(\n",
                "        lambda x:x>0,w.data.cpu().numpy()))][:l.data.cpu().item()]))[8:]\n",
                "to_str_gt = lambda w: str(\" \".join([info['ind2word'][x] for x in filter(\n",
                "        lambda x:x>0,w.data.cpu().numpy())]))[8:-6]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 250,
            "metadata": {},
            "outputs": [],
            "source": [
                "def var_map(tensor):\n",
                "    return Variable(tensor.unsqueeze(0), volatile=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 257,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\anest\\AppData\\Local\\Temp\\ipykernel_33656\\881476409.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
                        "  return Variable(tensor.unsqueeze(0), volatile=True)\n"
                    ]
                }
            ],
            "source": [
                "doc = \"A vase of flowers sitting on table\"\n",
                "docs = doc\n",
                "summary = 'A vase of flowers sitting on table'\n",
                "\n",
                "words = nltk.word_tokenize(doc)\n",
                "words = [word.lower() for word in words if word.isalpha()]\n",
                "doc = ' '.join(words)\n",
                "\n",
                "words = nltk.word_tokenize(summary)\n",
                "words = [word.lower() for word in words if word.isalpha()]\n",
                "summary = ' '.join(words)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Process document\n",
                "doc_tokens = tokenize(doc)\n",
                "doc = ind_map(doc_tokens)\n",
                "\n",
                "# Process document\n",
                "summ_tokens = tokenize(summary)\n",
                "summ = ind_map(summ_tokens)\n",
                "\n",
                "\n",
                "doc_tensor = var_map(torch.from_numpy(doc))\n",
                "doc_lens = var_map(torch.LongTensor([len(doc)]))\n",
                "\n",
                "summ_tensor = var_map(torch.from_numpy(summ))\n",
                "summ_lens = var_map(torch.LongTensor([len(summ)]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 258,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\anest\\AppData\\Local\\Temp\\ipykernel_33656\\881476409.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
                        "  return Variable(tensor.unsqueeze(0), volatile=True)\n"
                    ]
                }
            ],
            "source": [
                "q = \"how is hemophilia treated ?\"\n",
                "\n",
                "words = nltk.word_tokenize(q)\n",
                "words = [word.lower() for word in words if word.isalpha()]\n",
                "q = ' '.join(words)\n",
                "\n",
                "q_tokens = tokenize(q)\n",
                "q = ind_map(q_tokens)\n",
                "\n",
                "q_tensor = var_map(torch.from_numpy(q))\n",
                "q_lens = var_map(torch.LongTensor([len(q)]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 259,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "A1:  white <END> <END>\n",
                        "A1:  white <END>\n",
                        "A1:  white <END>\n"
                    ]
                }
            ],
            "source": [
                "if aBot:\n",
                "     aBot.eval(), aBot.reset()\n",
                "     aBot.train(), aBot.reset()\n",
                "     aBot.observe(-1, summary=summ_tensor, summaryLens=summ_lens, document=doc_tensor,\n",
                "                    documentLens=doc_lens)\n",
                "    \n",
                "     aBot.observe(0, ques=q_tensor, quesLens=q_lens)\n",
                "     answers, ansLens = aBot.forwardDecode(\n",
                "         inference='greedy', beamSize=15)\n",
                "     aBot.observe(0, ans=answers, ansLens=ansLens)\n",
                "     print(\"A%d: \"%(0+1), to_str_gt(answers[0]))\n",
                "     \n",
                "     aBot.observe(1, ques=q_tensor, quesLens=q_lens)\n",
                "     answers, ansLens = aBot.forwardDecode(\n",
                "         inference='greedy', beamSize=15)\n",
                "     aBot.observe(1, ans=answers, ansLens=ansLens)\n",
                "     print(\"A%d: \"%(0+1), to_str_gt(answers[0]))\n",
                "     \n",
                "     aBot.observe(2, ques=q_tensor, quesLens=q_lens)\n",
                "     answers, ansLens = aBot.forwardDecode(\n",
                "         inference='greedy', beamSize=15)\n",
                "     aBot.observe(2, ans=answers, ansLens=ansLens)\n",
                "     print(\"A%d: \"%(0+1), to_str_gt(answers[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 264,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Q1:  is ? <END> <END> <END>\n",
                        "A1:  yes <END>\n",
                        "\n",
                        "Q2:  is ? ? ? ? <END>\n",
                        "A2:  yes <END>\n",
                        "\n",
                        "Q3:  is ? ? ? ? <END>\n",
                        "A3:  yes <END>\n",
                        "\n",
                        "Q4:  is ? ? ? ? <END>\n",
                        "A4:  yes <END>\n",
                        "\n",
                        "Q5:  is ? ? ? ? <END>\n",
                        "A5:  yes <END>\n",
                        "\n",
                        "Q6:  is ? ? ? ? <END>\n",
                        "A6:  yes <END>\n",
                        "\n",
                        "Q7:  is ? ? ? ? <END>\n",
                        "A7:  yes <END>\n",
                        "\n",
                        "Q8:  is ? ? ? ? <END>\n",
                        "A8:  yes <END>\n",
                        "\n",
                        "Q9:  is ? ? ? ? <END>\n",
                        "A9:  yes <END>\n",
                        "\n",
                        "Q10:  is ? ? ? ? <END>\n",
                        "A10:  yes <END>\n",
                        "\n",
                        "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
                        "----------------------------------------\n",
                        "Summary:  number cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah cheetah c\n"
                    ]
                }
            ],
            "source": [
                "if aBot:\n",
                "    aBot.eval(), aBot.reset()\n",
                "    aBot.train(), aBot.reset()\n",
                "    aBot.observe(-1, summary=summ_tensor, summaryLens=summ_lens, document=doc_tensor,\n",
                "                    documentLens=doc_lens)\n",
                "\n",
                "if qBot:\n",
                "    qBot.eval(), qBot.reset()\n",
                "    qBot.observe(-1, document=doc_tensor,\n",
                "                    documentLens=doc_lens)\n",
                "\n",
                "numRounds = 10\n",
                "beamSize = 5\n",
                "\n",
                "for round in range(numRounds):\n",
                "    questions, quesLens = qBot.forwardDecode(\n",
                "         inference='greedy', beamSize=beamSize)\n",
                "    qBot.observe(round, ques=questions, quesLens=quesLens)\n",
                "    aBot.observe(round, ques=questions, quesLens=quesLens)\n",
                "    answers, ansLens = aBot.forwardDecode(\n",
                "         inference='greedy', beamSize=beamSize)\n",
                "    aBot.observe(round, ans=answers, ansLens=ansLens)\n",
                "    qBot.observe(round, ans=answers, ansLens=ansLens)\n",
                "    \n",
                "    # Printing\n",
                "    print(\"Q%d: \"%(round+1), to_str_gt(questions[0]))\n",
                "    print(\"A%d: \"%(round+1), to_str_gt(answers[0]))\n",
                "    print('')\n",
                "\n",
                "\n",
                "summ, summLens = qBot.predictSummary(inference='greedy', beamSize=5)\n",
                "print(Rouge().get_scores(summary, to_str_gt(summ[0]), avg=True))\n",
                "print('----------------------------------------')\n",
                "print(\"Summary: \", to_str_gt(summ[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 150,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
                            " 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
                            " 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}"
                        ]
                    },
                    "execution_count": 150,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Rouge().get_scores(summary, to_str_gt(summ[0]), avg=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'rouge-1': {'r': 0.010857763300760043,\n",
                            "  'p': 0.2777777777777778,\n",
                            "  'f': 0.0208986408642473},\n",
                            " 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
                            " 'rouge-l': {'r': 0.009771986970684038, 'p': 0.25, 'f': 0.018808776705417623}}"
                        ]
                    },
                    "execution_count": 49,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Rouge().get_scores(to_str_gt(summ[0]), docs, avg=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "well\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "for word in summary.split(' '):\n",
                "    if word in to_str_gt(summ[0]).split(' '):\n",
                "        print(word)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lgenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "54a2976d16502cff2e17b6fe6532cd24781aa51a139832d843a9c81ff15d5592"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
