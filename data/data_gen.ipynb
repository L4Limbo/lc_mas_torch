{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":293,"status":"ok","timestamp":1670756915652,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"},"user_tz":-120},"id":"Bn3XSIsrToMk"},"outputs":[],"source":["# Mount Google Drive\n","import os\n","os.chdir(\"/content/\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2454,"status":"ok","timestamp":1670752548669,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"},"user_tz":-120},"id":"A6NZRLUdQpUl","outputId":"3ca247da-6c6f-457f-8704-f17cd78d2ecb"},"outputs":[],"source":["!pip install -U transformers==3.0.0\n","!python -m nltk.downloader punkt\n","!git clone https://github.com/patil-suraj/question_generation.git"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":720,"status":"ok","timestamp":1670698232309,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"},"user_tz":-120},"id":"AUPbKN5aUwuw","outputId":"f3592126-6ea3-4d0a-c771-4326d85e7856"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3293680,"status":"ok","timestamp":1670760213455,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"},"user_tz":-120},"id":"VljsUZ8DQckA","outputId":"dcb051a6-6dbb-4195-87df-5189e4172304"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pipeline ...\n","Loading dataset ...\n","Parsing dataset initiated ...\n","Ready to save dataset ...\n","Created generated dataset ...\n"]}],"source":["# Generate Data with MEDQA-Summarization Dataset\n","\n","\n","import json\n","import h5py\n","import numpy as np\n","import string\n","import re\n","import nltk\n","import random\n","import torch\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from question_generation.pipelines import pipeline\n","\n","\n","def generate_dataset(gen_pipeline: pipeline, data: json) -> None:\n","\n","    # Initialize Data Dictionary\n","    dataset = {}\n","    summary_dataset = {}\n","    dataKeys = data.keys()\n","    dataset['questions'], dataset['answers'], dataset['dialogs'] = [], [], []\n","\n","    summary_id = 0\n","\n","    print('Parsing dataset initiated ...')\n","    for key in dataKeys:\n","\n","        # Store questions' and answers' last ids\n","        questions_id_counter = len(dataset['questions'])\n","        answers_id_counter = len(dataset['answers'])\n","\n","        # Append Question\n","        dataset['questions'].append(data[key]['question'])\n","\n","        # Append Answers\n","        answer_keys = list(data[key]['answers'].keys())\n","\n","        for answer_key in answer_keys:\n","            dataset['answers'].append(\n","                data[key]['answers'][answer_key]['answer_ext_summ'])\n","\n","        # Generate Dialogs from dataset\n","        current_dialog = {\n","            'summary': '',\n","            'document': '',\n","            'dialog': [],\n","        }\n","\n","        # Set summary and save it to summaries' dataset\n","        summary_dataset[str(summary_id)] = data[key]['multi_ext_summ']\n","        current_dialog['summary'] = str(summary_id)\n","        summary_id += 1\n","\n","        # Set the first article as the dialog's document\n","        current_dialog['document'] = data[key]['answers'][answer_keys[0]]['article']\n","\n","        # Generate Dialogues from the standard dataset\n","        dialogue = {\n","            'question': questions_id_counter,\n","            'answer': answers_id_counter,\n","            'answer_options': [i for i in range(answers_id_counter, len(dataset['answers']))],\n","            'gt_index': 0,\n","        }\n","        dialogue['gt_index'] = 0\n","\n","        current_dialog['dialog'].append(dialogue)\n","\n","        # Generate more questions and answers with transformer pipeline\n","        for answer_key in answer_keys:\n","            try:\n","              # Generate questions and answers for each article in the answers\n","              generated_qas = gen_pipeline(data[key]['answers'][answer_key]['article'])\n","            except:\n","                continue\n","\n","            # Append Results in the generated dataset and Create new dialogues\n","            prev_answer_id_counter = answers_id_counter\n","            for qa in generated_qas:\n","\n","                questions_id_counter = len(dataset['questions'])\n","                answers_id_counter = len(dataset['answers'])\n","\n","                dataset['questions'].append(qa['question'])\n","                dataset['answers'].append(qa['answer'])\n","\n","                # Generate Dialogues from generated question-answers\n","              \n","                dialogue = {\n","                    'question': len(dataset['questions']) - 1,\n","                    'answer': len(dataset['answers']) - 1,\n","                    'answer_options': [i for i in range(prev_answer_id_counter, len(dataset['answers']))],\n","                    'gt_index': 0,\n","                }\n","                dialogue['gt_index'] = len(dialogue['answer_options']) - 1\n","\n","                current_dialog['dialog'].append(dialogue)\n","\n","        dataset['dialogs'].append(current_dialog)\n","\n","    print('Ready to save dataset ...')\n","    dataset_to_create = {\n","        'data': dataset\n","    }\n","\n","    with open('drive/MyDrive/gen_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(dataset_to_create, indent=4))\n","\n","    with open('drive/MyDrive/summary_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(summary_dataset, indent=4))\n","      \n","\n","    print('Created generated dataset ...')\n","\n","\n","def main():\n","    # Question Driven Answer Summarization Primary Dataset path\n","    mediqa_ans_summ_dataset_path = 'drive/MyDrive/question_driven_answer_summarization_primary_dataset.json'\n","\n","    # Load Pipeline for QA Generation\n","    print('Loading pipeline ...')\n","    qa_gen_pipeline = pipeline(\n","        'multitask-qa-qg', model=\"valhalla/t5-base-qa-qg-hl\")\n","\n","    # Generate Dataset with VisDial-like structure\n","    print('Loading dataset ...')\n","    jsonData = json.load(open(mediqa_ans_summ_dataset_path))\n","\n","    generate_dataset(qa_gen_pipeline, jsonData)\n","\n","    pass\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate Data with part of XSUM\n","\n","\n","import json\n","import h5py\n","import numpy as np\n","import string\n","import re\n","import nltk\n","import random\n","import torch\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from question_generation.pipelines import pipeline\n","\n","\n","def generate_xsum(xsum, gen_pipeline):\n","    dataset = {}\n","    summary_dataset = {}\n","    dataset['questions'], dataset['answers'], dataset['dialogs'] = [], [], []\n","    summary_id = 0\n","    print('Parsing dataset initiated ...')\n","    \n","    for i in range(0, 20332):\n","        if i % 100 == 0:\n","            dataset_to_create = {\n","                'data': dataset\n","            }\n","\n","            with open('drive/MyDrive/xgen_dataset.json', 'w') as jsonFile:\n","                jsonFile.write(json.dumps(dataset_to_create, indent=4))\n","\n","            with open('drive/MyDrive/xsummary_dataset.json', 'w') as jsonFile:\n","                jsonFile.write(json.dumps(summary_dataset, indent=4))\n","\n","        # Generate Dialogues from the standard dataset\n","        pair = xsum['train'][i]\n","\n","        try:\n","            generated_qas = gen_pipeline(pair['document'])\n","        except:\n","            continue\n","        \n","        # Store questions' and answers' last ids\n","        questions_id_counter = len(dataset['questions'])\n","        answers_id_counter = len(dataset['answers'])\n","        \n","        # Generate Dialogs from dataset\n","        current_dialog = {\n","            'summary': '',\n","            'document': '',\n","            'dialog': [],\n","        }\n","\n","        # Set summary and save it to summaries' dataset\n","        summary_dataset[str(summary_id)] = pair['summary']\n","        current_dialog['summary'] = str(summary_id)\n","        summary_id += 1\n","        \n","        # Set document\n","        current_dialog['document'] = pair['document']\n","        \n","        prev_answer_id_counter = answers_id_counter\n","        for qa in generated_qas:\n","\n","            questions_id_counter = len(dataset['questions'])\n","            answers_id_counter = len(dataset['answers'])\n","\n","            dataset['questions'].append(qa['question'])\n","            dataset['answers'].append(qa['answer'])\n","\n","            # Generate Dialogues from generated question-answers\n","            \n","            dialogue = {\n","                'question': len(dataset['questions']) - 1,\n","                'answer': len(dataset['answers']) - 1,\n","                'answer_options': [i for i in range(prev_answer_id_counter, len(dataset['answers']))],\n","                'gt_index': 0,\n","            }\n","            dialogue['gt_index'] = len(dialogue['answer_options']) - 1\n","\n","            current_dialog['dialog'].append(dialogue)\n","        \n","        dataset['dialogs'].append(current_dialog)\n","        \n","    print('Ready to save dataset ...')\n","    dataset_to_create = {\n","        'data': dataset\n","    }\n","\n","    with open('drive/MyDrive/xgen_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(dataset_to_create, indent=4))\n","\n","    with open('drive/MyDrive/xsummary_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(summary_dataset, indent=4))\n","      \n","\n","    print('Created generated dataset ...')\n","    \n","    \n","    \n","def main():\n","    # XSUM Dataset \n","    print('Loading dataset ...')\n","    from datasets import load_dataset\n","    dataset = load_dataset(\"xsum\")\n","\n","    # Load Pipeline for QA Generation\n","    print('Loading pipeline ...')\n","    qa_gen_pipeline = pipeline(\n","        'multitask-qa-qg', model=\"valhalla/t5-base-qa-qg-hl\")\n","\n","    # Generate Dataset with VisDial-like structure\n","    generate_xsum(dataset, qa_gen_pipeline)\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ensure Answer-Options Lengths\n","import json \n","import random \n","\n","xsum = json.load(open('drive/MyDrive/xgen_dataset.json'))['data']\n","medqa = json.load(open('drive/MyDrive/gen_dataset.json'))['data']\n","\n","for dialogue in medqa['dialogs']:\n","    for dialog in dialogue['dialog']:\n","        while len(dialog['answer_options']) < 100:\n","            dialog['answer_options'].append(random.randrange(0, len(medqa['answers'])))\n","\n","with open('drive/MyDrive/gen_dataset_new.json', 'w') as jsonFile:\n","    jsonFile.write(json.dumps({'data': medqa}, indent=4))\n","\n","print('medqa finished')\n","\n","for dialogue in xsum['dialogs']:\n","    for dialog in dialogue['dialog']:\n","        while len(dialog['answer_options']) < 100:\n","            dialog['answer_options'].append(random.randrange(0, len(xsum['answers'])))\n","\n","with open('drive/MyDrive/xgen_dataset_new.json', 'w') as jsonFile:\n","    jsonFile.write(json.dumps({'data': xsum}, indent=4))\n","\n","print('xsum finished')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP8qHkc1ZmqH+eBy2d+iLFC","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"lgenv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"54a2976d16502cff2e17b6fe6532cd24781aa51a139832d843a9c81ff15d5592"}}},"nbformat":4,"nbformat_minor":0}
