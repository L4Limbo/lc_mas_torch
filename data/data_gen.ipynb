{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHhGSS8uKkj+A5ZFd4MF6H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import os\n","os.chdir(\"/content/\")"],"metadata":{"id":"Bn3XSIsrToMk","executionInfo":{"status":"ok","timestamp":1670752494481,"user_tz":-120,"elapsed":6,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHQd6gFDWPfD","executionInfo":{"status":"ok","timestamp":1670752521053,"user_tz":-120,"elapsed":19781,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"c89a6fb8-fe03-4204-cd88-1b649db75022"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -U transformers==3.0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C0Oq_aMQi-x","executionInfo":{"status":"ok","timestamp":1670752546218,"user_tz":-120,"elapsed":7230,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"881b2f03-a9a5-4419-c05c-4caff3c80ba1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.0\n","  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n","\u001b[K     |████████████████████████████████| 754 kB 24.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (2022.6.2)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 72.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (4.64.1)\n","Collecting tokenizers==0.8.0-rc4\n","  Downloading tokenizers-0.8.0rc4-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 67.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=03cb023f79a3f4ee9fcc80de73d60355e70c0aa634873d801f9762f54396984f\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.0rc4 transformers-3.0.0\n"]}]},{"cell_type":"code","source":["!python -m nltk.downloader punkt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6NZRLUdQpUl","executionInfo":{"status":"ok","timestamp":1670752548669,"user_tz":-120,"elapsed":2454,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"3ca247da-6c6f-457f-8704-f17cd78d2ecb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUPbKN5aUwuw","executionInfo":{"status":"ok","timestamp":1670698232309,"user_tz":-120,"elapsed":720,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"f3592126-6ea3-4d0a-c771-4326d85e7856"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["!git clone https://github.com/patil-suraj/question_generation.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFK813wSQtFJ","executionInfo":{"status":"ok","timestamp":1670752549170,"user_tz":-120,"elapsed":504,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"3292c1ee-a502-4098-81d7-1f6707fe5c2a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'question_generation'...\n","remote: Enumerating objects: 268, done.\u001b[K\n","remote: Total 268 (delta 0), reused 0 (delta 0), pack-reused 268\u001b[K\n","Receiving objects: 100% (268/268), 299.04 KiB | 8.54 MiB/s, done.\n","Resolving deltas: 100% (140/140), done.\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VljsUZ8DQckA","executionInfo":{"status":"ok","timestamp":1670755917167,"user_tz":-120,"elapsed":1999628,"user":{"displayName":"Anestis Mitakidis","userId":"03873543257711321867"}},"outputId":"88790cf6-f536-4244-e565-9880bac57c5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pipeline ...\n","Loading dataset ...\n","Parsing dataset initiated ...\n","Ready to save dataset ...\n","Created generated dataset ...\n"]}],"source":["import json\n","import h5py\n","import numpy as np\n","import string\n","import re\n","import nltk\n","import random\n","import torch\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from question_generation.pipelines import pipeline\n","\n","\n","def generate_dataset(gen_pipeline: pipeline, data: json) -> None:\n","\n","    # Initialize Data Dictionary\n","    dataset = {}\n","    summary_dataset = {}\n","    dataKeys = data.keys()\n","    dataset['questions'], dataset['answers'], dataset['dialogs'] = [], [], []\n","\n","    summary_id = 0\n","\n","    print('Parsing dataset initiated ...')\n","    for key in dataKeys:\n","\n","        # Store questions' and answers' last ids\n","        questions_id_counter = len(dataset['questions'])\n","        answers_id_counter = len(dataset['answers'])\n","\n","        # Append Question\n","        dataset['questions'].append(data[key]['question'])\n","\n","        # Append Answers\n","        answer_keys = list(data[key]['answers'].keys())\n","\n","        for answer_key in answer_keys:\n","            dataset['answers'].append(\n","                data[key]['answers'][answer_key]['answer_ext_summ'])\n","\n","        # Generate Dialogs from dataset\n","        current_dialog = {\n","            'summary': '',\n","            'document': '',\n","            'dialog': [],\n","        }\n","\n","        # Set summary and save it to summaries' dataset\n","        summary_dataset[str(summary_id)] = data[key]['multi_ext_summ']\n","        current_dialog['summary'] = str(summary_id)\n","        summary_id += 1\n","\n","        # Set the first article as the dialog's document\n","        current_dialog['document'] = data[key]['answers'][answer_keys[0]]['article']\n","\n","        # Generate Dialogues from the standard dataset\n","        dialogue = {\n","            'question': str(questions_id_counter),\n","            'answer': str(answers_id_counter),\n","            'answer_options': [i for i in range(answers_id_counter, len(dataset['answers']))],\n","            'gt_index': '',\n","        }\n","        dialogue['gt_index'] = '0'\n","\n","        current_dialog['dialog'].append(dialogue)\n","\n","        # Generate more questions and answers with transformer pipeline\n","        for answer_key in answer_keys:\n","            try:\n","              # Generate questions and answers for each article in the answers\n","              generated_qas = gen_pipeline(data[key]['answers'][answer_key]['article'])\n","            except:\n","                continue\n","\n","            # Append Results in the generated dataset and Create new dialogues\n","            prev_answer_id_counter = answers_id_counter\n","            for qa in generated_qas:\n","\n","                questions_id_counter = len(dataset['questions'])\n","                answers_id_counter = len(dataset['answers'])\n","\n","                dataset['questions'].append(qa['question'])\n","                dataset['answers'].append(qa['answer'])\n","\n","                # Generate Dialogues from generated question-answers\n","              \n","                dialogue = {\n","                    'question': str(len(dataset['questions']) - 1),\n","                    'answer': str(len(dataset['answers']) - 1),\n","                    'answer_options': [i for i in range(prev_answer_id_counter, len(dataset['answers']))],\n","                    'gt_index': '',\n","                }\n","                dialogue['gt_index'] = str(len(dialogue['answer_options']) - 1)\n","\n","                current_dialog['dialog'].append(dialogue)\n","\n","        dataset['dialogs'].append(current_dialog)\n","\n","    print('Ready to save dataset ...')\n","    dataset_to_create = {\n","        'data': dataset\n","    }\n","\n","    with open('drive/MyDrive/gen_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(dataset_to_create, indent=4))\n","\n","    with open('drive/MyDrive/summary_dataset.json', 'w') as jsonFile:\n","        jsonFile.write(json.dumps(summary_dataset, indent=4))\n","      \n","\n","    print('Created generated dataset ...')\n","\n","\n","def main():\n","    # Question Driven Answer Summarization Primary Dataset path\n","    mediqa_ans_summ_dataset_path = 'drive/MyDrive/question_driven_answer_summarization_primary_dataset.json'\n","\n","    # Load Pipeline for QA Generation\n","    print('Loading pipeline ...')\n","    qa_gen_pipeline = pipeline(\n","        'multitask-qa-qg', model=\"valhalla/t5-base-qa-qg-hl\")\n","\n","    # Generate Dataset with VisDial-like structure\n","    print('Loading dataset ...')\n","    jsonData = json.load(open(mediqa_ans_summ_dataset_path))\n","\n","    generate_dataset(qa_gen_pipeline, jsonData)\n","\n","    pass\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"F48P1iMRUwoJ"}}]}