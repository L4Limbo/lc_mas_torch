{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import matutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocabulary and Vectors\n"
     ]
    }
   ],
   "source": [
    "print('Loading Vocabulary and Vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "    'data/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "vocabulary = json.load(open('data/processed_data/processed_data.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6194, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.get_scores(\n",
    "    generated_summary, reference_summary\n",
    ")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_emb(sequence, word2vec, vocabulary):\n",
    "    \n",
    "    seq_tokens = []\n",
    "    \n",
    "    for key in sequence:\n",
    "        try:\n",
    "            seq_tokens.append(vocabulary['ind2word'][str(key.item())])\n",
    "        except:\n",
    "            seq_tokens.append(vocabulary['ind2word'][list(\n",
    "                vocabulary['ind2word'].keys())[-1]])\n",
    "    \n",
    "    vectorized = []\n",
    "    for word in seq_tokens:\n",
    "        try:\n",
    "            vectorized.append(word2vec[word])\n",
    "        except:\n",
    "            vectorized.append(np.zeros(300,))\n",
    "    \n",
    "    return np.array(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "    print(cosine_similarity)\n",
    "    print(spatial.distance.cosine(vec1, vec2))\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(target, generated, word2vec, vocabulary):\n",
    "    '''\n",
    "    Calculate the reward for the generated text\n",
    "    '''\n",
    "    target = word2vec_emb(target, word2vec, vocabulary)\n",
    "    generated = word2vec_emb(generated, word2vec, vocabulary)\n",
    "    \n",
    "    reward = similarity_cosine(target.mean(axis=0), generated.mean(axis=0))\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = torch.tensor([[6100, 4948, 6066, 1336, 4264, 4093,   37, 3343, 2467, 5527, 3396, 4326,\n",
    "          219, 5560, 5845, 1464, 3149, 2805, 4154, 1899, 5642]])\n",
    "\n",
    "question = torch.tensor([[6100, 2440, 5128, 5570,  511, 2483, 4828, 3564,  282, 3869, 2614,    7,\n",
    "         4783, 4595, 5613, 1385, 6032, 4369,  286, 2259, 1900]])\n",
    "\n",
    "summary = torch.tensor([[6100, 5955, 2642, 6021, 1982, 3081, 3554, 3802, 4577, 5298, 5555, 1110,\n",
    "         4515, 2661, 2299, 4606, 4956, 4007, 3281, 2625, 2206, 4335, 2730, 2925,\n",
    "         4523, 1231, 4269,  427,  861, 4104, 2630, 2499, 5430, 4043, 5146, 4595,\n",
    "         3772, 5545, 4852, 3325, 4324]])\n",
    "\n",
    "gtSummary = torch.tensor([[405,  333,  880,  188,  624,  194,  996,  293,  244, 6099, 3275,   99,\n",
    "           25,  244,  754,  636,  936, 1461, 3653,   51,   55,   56,   57,    1,\n",
    "           58,    1,   59,    1,   60,   52,  244, 1367, 6099, 1368, 3654, 2424,\n",
    "           76,   51, 3655,   52]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565139467498903\n",
      "0.6523351061715779\n"
     ]
    }
   ],
   "source": [
    "# torch.cat((answer[0], question[0]))\n",
    "print(reward(target=torch.cat((answer[0], question[0])), generated=summary[0], word2vec=word2vec, vocabulary=vocabulary))\n",
    "print(reward(target=gtSummary[0], generated=summary[0], word2vec=word2vec, vocabulary=vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "def rouge_scores(target, generated, word2vec, vocabulary):\n",
    "    rouge = Rouge()\n",
    "    tar = []\n",
    "    ref = []\n",
    "\n",
    "    for key in target:\n",
    "        try:\n",
    "            if vocabulary['ind2word'][str(key.item())] !='UNK':\n",
    "                tar.append(vocabulary['ind2word'][str(key.item())])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for key in generated:\n",
    "        try:\n",
    "            if vocabulary['ind2word'][str(key.item())] !='UNK':\n",
    "                ref.append(vocabulary['ind2word'][str(key.item())])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return rouge.get_scores(' '.join(ref), ' '.join(tar), avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 visible one-half calling retinal open-angle missing elbows street hopeless absolutely S loss Depakene TEE acid minimal double recurrences opposite Cutting tends efforts settings hepatitis reflexes heart-lung Name 1 Oxygen endoscopy if Autonomic beat companies cervical Nonne-Milroy separates Fragile osteochondrodysplasias abnormally\n",
      "- - - - - - \n",
      "40 running electrical Recurrent barley amyloid Rapid senses containers Evidence Hospitalization testing Joseph FODMAP x-rays fasting coated Southeast psoralen Sharing natural G pacing mouthwashes Rasagiline While psychiatric resistance new lights relieving babies Involved specially firm beat relationships instability cookies colloidal mechanisms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.025, 'p': 0.025, 'f': 0.024999995000001003},\n",
       " 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       " 'rouge-l': {'r': 0.025, 'p': 0.025, 'f': 0.024999995000001003}}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores(torch.cat((answer[0], question[0])), summary[0], word2vec, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.7142857142857143, 'p': 1.0, 'f': 0.8333333284722222},\n",
       " 'rouge-2': {'r': 0.6666666666666666, 'p': 1.0, 'f': 0.7999999952000001},\n",
       " 'rouge-l': {'r': 0.7142857142857143, 'p': 1.0, 'f': 0.8333333284722222}}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rouge().get_scores('I go to the beach','I go to the beach every data', avg=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54a2976d16502cff2e17b6fe6532cd24781aa51a139832d843a9c81ff15d5592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
